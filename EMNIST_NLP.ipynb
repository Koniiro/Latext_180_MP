{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHCCGutqrj-r"
   },
   "source": [
    "# ‚öôÔ∏è Imports and Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiFYfrVpXbAP",
    "outputId": "620dcba0-66f4-428d-e265-17fd20fa8247"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  7 18:31:32 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 497.29       Driver Version: 497.29       CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   52C    P8     6W /  N/A |    134MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGY8OiyGpN-x",
    "outputId": "f9c3a1e5-0e1a-40a7-86de-e99b497c304a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import csv\n",
    "import string\n",
    "import pathlib\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from itertools import groupby\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9VJbZx9g9Uz"
   },
   "source": [
    "# ‚ô£Ô∏è Download Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHw1hCpYg_1R"
   },
   "source": [
    "#### ‚úîÔ∏è Wiki-split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4SOyIXEHnI4p",
    "outputId": "acd3dd12-6f79-4fa1-e421-d149a5a8a2bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'wiki-split' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/google-research-datasets/wiki-split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ewq14NyxhId2"
   },
   "source": [
    "> Side Note: We will be using `test.tsv` for our experiment to speed up the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcRbRClXjBq1"
   },
   "source": [
    "#### ‚úîÔ∏è EMNIST/Bymerge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0qrzBv-XjFiQ",
    "outputId": "68c34290-0122-4494-c8b7-dd2b6385ed9b"
   },
   "outputs": [],
   "source": [
    "# Gather EMNIST/bymerge dataset\n",
    "train_ds, validation_ds = tfds.load(\n",
    "    \"emnist/byclass\",\n",
    "    split=[\"train[:85%]\", \"train[85%:]\"],\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Of-n4NypN_4"
   },
   "source": [
    "# ‚öóÔ∏è Load Data, Clean it and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Av4Db7lpN_8",
    "outputId": "cee1ef04-7ae1-4e95-a076-6b617010b39f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:00, 192581.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = []  # empty list to store sentences\n",
    "with open(\"wiki-split/test.tsv\", \"r\", encoding=\"utf8\") as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')  # read a tsv file\n",
    "    for row in tqdm(reader):\n",
    "        raw_sentences.extend(row[1].split(\"<::::>\"))\n",
    "print(\"Total Sentences: \", len(raw_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0C-5foj6pOAN",
    "outputId": "8c7dd611-ed03-4f8f-f41f-9893c711e374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " He was was arrested and booked on charges of first - degree murder and first - degree robbery . \n",
      " A 2006 estimate by the International Organization for Migration put the number of Sudanese people in the UK at a much higher figure . \n"
     ]
    }
   ],
   "source": [
    "# Some samples\n",
    "print(raw_sentences[99], '\\n', raw_sentences[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIPqzykIkXxd"
   },
   "source": [
    "#### ‚úîÔ∏è Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HtS5z3MpOAe",
    "outputId": "1ee54628-fc65-487a-db15-d373d6d27db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was was arrested and booked on charges of first degree murder and first degree robbery \n",
      "  He was was arrested and booked on charges of first - degree murder and first - degree robbery .\n"
     ]
    }
   ],
   "source": [
    "# As we see there are lots of punctuations which we dont have in EMNIST, so we are going to remove them, \n",
    "# and replace multiple spaces with one\n",
    "import re\n",
    "\n",
    "sentences = []\n",
    "table = str.maketrans({key: None for key in string.punctuation})  # translation table\n",
    "\n",
    "for sentence in raw_sentences:\n",
    "    # remove punctuation and non-ascii characters\n",
    "    clean_sentence = re.sub('  +', ' ', sentence.translate(table)).\\\n",
    "                        encode(\"ascii\", 'ignore').decode()  \n",
    "    sentences.append(clean_sentence.strip())  # add to clean sentences\n",
    "    \n",
    "print(sentences[99], '\\n', raw_sentences[99])  # to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4DxGdQ087p1V",
    "outputId": "8c77bf94-a46d-4a9b-e1cc-72a6ff67bed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences 10000\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentences', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwUb0KtemeK3"
   },
   "source": [
    "#### ‚úîÔ∏è EMNIST Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSXgQqsOEF38",
    "outputId": "5cc0a4a2-2976-4c07-8059-15b89341cd5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "MAPPINGS = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "          'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "          'a', 'b', 'd', 'e', 'f', 'g', 'h', 'n', 'q', 'r', 't']\n",
    "\n",
    "print(len(MAPPINGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USfu5WfonDPZ"
   },
   "source": [
    "#### ‚úîÔ∏è Dataloader for EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WE7sVKVPEJzK"
   },
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "## We are transposing to rotate the image by 90 deg clockwise making the images human friendly.\n",
    "def transpose(image, label):\n",
    "  image = tf.image.convert_image_dtype(image, dtype=tf.float32) # scale image pixels to [0,1]\n",
    "  image = tf.transpose(image, [1,0,2]) # transpose to get human friendly image, since rotation\n",
    "  return image, label\n",
    "\n",
    "trainloader = (\n",
    "    train_ds\n",
    "    .shuffle(1024)\n",
    "    .map(transpose, num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "Exo6ceZ1EJv0",
    "outputId": "b3501978-ad19-46ca-cf94-d04ee42df89c"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(tf\u001b[38;5;241m.\u001b[39mreshape(img, (\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m)), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[43mMAPPINGS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 11 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "n = 0\n",
    "for img, label in trainloader.take(25):\n",
    "    ax = plt.subplot(5, 5, n+1)\n",
    "    plt.imshow(tf.reshape(img, (28,28)), cmap='gray')\n",
    "    plt.title(MAPPINGS[int(label.numpy())])\n",
    "    plt.axis('off')\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiVoXehQnsOB"
   },
   "source": [
    "#### ‚úîÔ∏è Get all the images in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BZDQWDjRlXv",
    "outputId": "6c678df4-123e-4e01-b215-6a323c14d907"
   },
   "outputs": [],
   "source": [
    "IMAGES = []\n",
    "LABELS = []\n",
    "\n",
    "for img, label in tqdm(trainloader):\n",
    "    IMAGES.append(tf.reshape(img, (28,28)).numpy())\n",
    "    LABELS.append(label.numpy())\n",
    "\n",
    "IMAGES = np.array(IMAGES)\n",
    "LABELS = np.array(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75WCj0tmqIwo"
   },
   "source": [
    "#### ‚úîÔ∏è Index Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Th2nVHDvVYiG"
   },
   "outputs": [],
   "source": [
    "image_index = {}  # where key is the char and value is a list of IDs\n",
    "for idx, label in tqdm(enumerate(LABELS)):\n",
    "    char = MAPPINGS[label]\n",
    "    if char in image_index:\n",
    "        # this character already exists\n",
    "        image_index[char].append(idx) # append index\n",
    "    else:\n",
    "        image_index[char] = [idx]  # initiate list with 1 item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMWtqzPdqbO_"
   },
   "source": [
    "# üéµ Utils to generate image from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0bAc0kPXf9j"
   },
   "outputs": [],
   "source": [
    "def get_sample_sentences(sentences_in=sentences, num_sentence=10):\n",
    "    # Get a defined number of sentences from the data\n",
    "    return np.random.choice(sentences_in, num_sentence)\n",
    "\n",
    "def get_generated_image(words, chars=IMAGES, index=image_index):\n",
    "    # words is string of char/numbers that needs to be converted into an image\n",
    "    # chars is a data set of images that need to be used to compose, usually pass in train['features'] in here\n",
    "    # index maps a character to indexes in the images, available as dictionary\n",
    "    height, width = IMAGES[0].shape # height and width of each character\n",
    "    length = len(words) # total number of characters in the image\n",
    "    \n",
    "    # create an empty array to store the data\n",
    "    image = np.zeros((height, width * length))\n",
    "    pos = 0  # starting index of the character\n",
    "    \n",
    "    for char in words:\n",
    "        if char is ' ':\n",
    "            pos += width # if space, move over\n",
    "        else:\n",
    "            if char in image_index:\n",
    "                # pick a random item from all images for that char\n",
    "                idx = np.random.choice(image_index[char])  \n",
    "                image[:, pos:(pos+width)] += chars[idx]\n",
    "            elif char.upper() in image_index:  # to remove characters from other languages\n",
    "                # for some characters, there is only upper case\n",
    "                idx = np.random.choice(image_index[char.upper()])  \n",
    "                image[:, pos:(pos+width)] += chars[idx]\n",
    "            \n",
    "            pos += width\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASm9g8-_qsFR"
   },
   "source": [
    "#### ‚úîÔ∏è Check one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9buIC1YVX1g4"
   },
   "outputs": [],
   "source": [
    "# Let's print a sample to see how it looks\n",
    "s = get_sample_sentences(sentences, 1)\n",
    "print(s)\n",
    "img = get_generated_image(s[0])\n",
    "plt.figure(figsize=(9,2))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EzTHffnrEo4"
   },
   "source": [
    "# ‚õµ Build Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IEJhTF3YEg4"
   },
   "outputs": [],
   "source": [
    "train_sentences = sentences[:2000]\n",
    "test_sentences = sentences[2000:2500]\n",
    "\n",
    "# Lets assume that for each training sample, 2 variants will be generated\n",
    "\n",
    "def generate_sentences(texts, chars, \n",
    "                           index, num_variants=2, max_length=32):\n",
    "    # this method takes input text lines, character samples and labels\n",
    "    # and generates images. It can generate multiple images per sentence\n",
    "    # as controlled by num_variants parameter. max_length parameter\n",
    "    # ensures that all sentences are the same length\n",
    "    \n",
    "    # total number of samples to generate\n",
    "    num_samples = len(texts) * num_variants\n",
    "    height, width = chars[0].shape  # shape of image\n",
    "    \n",
    "    # setup empty array of the images\n",
    "    images = np.zeros((num_samples, height, width * max_length), np.float64)\n",
    "    labels = []\n",
    "    \n",
    "    for i, item in tqdm(enumerate(texts)):\n",
    "        padded_item = item[0:max_length] if (len(item) > max_length) else item.ljust(max_length, ' ')\n",
    "        \n",
    "        for v in range(num_variants):\n",
    "            img = get_generated_image(padded_item, chars, index)\n",
    "            images[i*num_variants+v, :, :] += img\n",
    "            labels.append(padded_item)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "317adGptYcs5"
   },
   "outputs": [],
   "source": [
    "train_images, train_labels = generate_sentences(train_sentences, IMAGES, image_index)\n",
    "test_images, test_labels = generate_sentences(test_sentences, IMAGES, image_index)\n",
    "\n",
    "train_images, train_labels = shuffle(train_images, train_labels)\n",
    "test_images, test_labels = shuffle(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgEJHI4CsUvY"
   },
   "source": [
    "#### ‚úîÔ∏è Check one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZvWNvk4Y1z_"
   },
   "outputs": [],
   "source": [
    "# Sample some to check\n",
    "idx=0\n",
    "print(train_labels[idx])\n",
    "plt.figure(figsize=(9,2))\n",
    "plt.imshow(train_images[idx], cmap='gray');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmgFAVAFr_1u"
   },
   "source": [
    "#### ‚úîÔ∏è Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiEyviXGa32C"
   },
   "outputs": [],
   "source": [
    "# Now to save these models for easy loading\n",
    "pp = pathlib.Path('.') / 'sentences'\n",
    "pp.mkdir(exist_ok=True)  # create the directory\n",
    "\n",
    "np.save(pp / 'train_images', train_images)\n",
    "np.save(pp / 'test_images', test_images)\n",
    "np.save(pp / 'train_labels', train_labels)\n",
    "np.save(pp / 'test_labels', test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8kQvdfl-Ox1"
   },
   "source": [
    "# üêã Prepare Training Dataset(Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDw__t1Jlk-8"
   },
   "outputs": [],
   "source": [
    "# create a dictionary of mapping from \n",
    "mappings = dict(zip(np.arange(0, 47), MAPPINGS))\n",
    "# add a space for spacing words\n",
    "mappings[47] = ' '\n",
    "# inverse: character as key and value as it's integer encoding\n",
    "inverse_mappings = {v: k for k, v in mappings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxCE4CgilaPA"
   },
   "outputs": [],
   "source": [
    "# now convert categorical labels from the sentences\n",
    "encode = lambda x: [inverse_mappings[xi] if xi in inverse_mappings else inverse_mappings[xi.upper()] for xi in x]\n",
    "decode = lambda x: [mappings[xi] for xi in x]\n",
    "\n",
    "train_labels_cat = np.array([encode(xi) for xi in train_labels])\n",
    "test_labels_cat = np.array([encode(xi) for xi in test_labels])\n",
    "\n",
    "# Verify the encoding/decoding\n",
    "print(train_labels[1])\n",
    "print(train_labels_cat.shape, train_labels_cat[1])\n",
    "print(decode(train_labels_cat[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "195rY50uPhrw"
   },
   "source": [
    "# üê≤ Build Train and Test Loader\n",
    "\n",
    "Using `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKNx2nN4Pqcc"
   },
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# This step will make sense after looking at the model\n",
    "def merge_image_label(image, label):\n",
    "    return (image, label), label\n",
    "\n",
    "trainloader = tf.data.Dataset.from_tensor_slices((train_images, train_labels_cat))\n",
    "testloader = tf.data.Dataset.from_tensor_slices((test_images, test_labels_cat))\n",
    "\n",
    "trainloader = (\n",
    "    trainloader\n",
    "    .shuffle(1024)\n",
    "    .map(merge_image_label, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "testloader = (\n",
    "    testloader\n",
    "    .map(merge_image_label, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xg-LdXSDsbPs"
   },
   "source": [
    "# üíé Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fdP1t6zuU73"
   },
   "outputs": [],
   "source": [
    "# CNN definition : LeNet5 architecture, a classic architecture\n",
    "IMG_H, IMG_W = train_images[1].shape\n",
    "# we limited output sentences to 32 chars\n",
    "OUTPUT_LENGTH = 32  \n",
    "# EMNIST has 47 labels, adding one for 'blank space'\n",
    "NUM_CLASSES = 49 # notice two additional class; one is for \"space\" another is required by \"CTCLayer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNeF-P0gsit7"
   },
   "source": [
    "#### ‚úîÔ∏è Utility to Extract Patch\n",
    "\n",
    "Since each image is a sentence, we will extract patches from it\n",
    "width of each patch will be 20 pixels wide (note EMNIST is 28 px)\n",
    "and every time we will move 14 px over to extract another sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfI1fixxumPn"
   },
   "outputs": [],
   "source": [
    "PATCH_WIDTH = 20\n",
    "PATCH_STRIDE = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RExyW0Opi_g_"
   },
   "outputs": [],
   "source": [
    "def extract_patches(image):\n",
    "    kernel = [1, 1, PATCH_WIDTH, 1]\n",
    "    strides = [1, 1, PATCH_STRIDE, 1]\n",
    "    patches = tf.image.extract_patches(image, kernel, strides, [1, 1, 1, 1], 'VALID')\n",
    "    patches = tf.transpose(patches, (0, 2, 1, 3))\n",
    "    patches = tf.expand_dims(patches, -1)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmjDDWLfHWyJ"
   },
   "source": [
    "#### ‚úîÔ∏è CNN backbone\n",
    "\n",
    "* Takes in a patch of image\n",
    "* Returned flattened embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ra0ehhwMG3FF"
   },
   "outputs": [],
   "source": [
    "def ImagePatchEncoder():\n",
    "  patched_inputs = Input(shape=(IMG_H, PATCH_WIDTH, 1))\n",
    "  x = Conv2D(32, kernel_size=(3, 3), activation='relu')(patched_inputs)\n",
    "  x = Conv2D(64, kernel_size=(3, 3), activation='relu')(x)\n",
    "  x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "  flattened_outputs = Flatten()(x)\n",
    "\n",
    "  return Model(inputs=patched_inputs, outputs=flattened_outputs, name='backbone')\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "image_patch_encoder = ImagePatchEncoder()\n",
    "image_patch_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezbWeXARO_C0"
   },
   "source": [
    "#### ‚úîÔ∏è CTC Loss Layer\n",
    "\n",
    "You can learn more about CTC Loss in this [awesome blog post](https://app.wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-with-CRNN-CTC-Network--VmlldzoxNTI5NDI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzyiMxNoH_vt"
   },
   "outputs": [],
   "source": [
    "## Ref: https://keras.io/examples/vision/captcha_ocr/\n",
    "class CTCLayer(Layer):\n",
    "  def __init__(self, name=None):\n",
    "\n",
    "      super().__init__(name=name)\n",
    "      self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
    "\n",
    "  def call(self, y_true, y_pred):\n",
    "      # Compute the training-time loss value and add it\n",
    "      # to the layer using `self.add_loss()`.\n",
    "\n",
    "      batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "      input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "      label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "      input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "      label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "      loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "      self.add_loss(loss)\n",
    "\n",
    "      # At test time, just return the computed predictions\n",
    "      return y_pred\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uun36GxI9G6V"
   },
   "source": [
    "#### ‚úîÔ∏è Final Model\n",
    "\n",
    "##### Image Encoder\n",
    "\n",
    "* Take in image input\n",
    "* Get patches of shape `(PATCH_HEIGHT, PATCH_WIDTH, 1)`.\n",
    "* Feed patches to backbone CNN architecture to get embedding. \n",
    "* Wrap using `TimeDistributed`\n",
    "\n",
    "\n",
    "##### Recurrent Network\n",
    "\n",
    "* Takes in `TimeDistributed` sequence\n",
    "* Computes output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tJu9Yn5NNIt"
   },
   "outputs": [],
   "source": [
    "def ImagetoSequence():\n",
    "  # IMAGE ENCODER #\n",
    "  labels = Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "  image_input = Input(shape=(IMG_H, IMG_W), name=\"cnn_input\")\n",
    "  # reshape to add dimensions\n",
    "  image_reshaped = Reshape((IMG_H, IMG_W, 1))(image_input)\n",
    "  # extract patches of images\n",
    "  image_patches = Lambda(extract_patches)(image_reshaped)\n",
    "  # get CNN backbone architecture to get embedding for each patch\n",
    "  image_patch_encoder = ImagePatchEncoder()\n",
    "  # Wrapper allows to apply a layer to every temporal slice of an input.\n",
    "  time_wrapper = TimeDistributed(image_patch_encoder)(image_patches)\n",
    "\n",
    "  # RECURRENT NETWORK #\n",
    "  lstm_out = LSTM(128, return_sequences=True, name=\"lstm\")(time_wrapper)\n",
    "  softmax_output = Dense(NUM_CLASSES, activation='softmax', name=\"lstm_softmax\")(lstm_out)\n",
    "  # \n",
    "  output = CTCLayer(name=\"ctc_loss\")(labels, softmax_output)\n",
    "\n",
    "  return Model([image_input, labels], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFEYPgl3Qihf"
   },
   "source": [
    "# üêß Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFnnEeoKIrYC"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = ImagetoSequence()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asdJgVutPOsD"
   },
   "source": [
    "# üê¢ Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAuhDm8jm-Vg"
   },
   "outputs": [],
   "source": [
    "# compile the models\n",
    "model.compile(tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeR-dUwePUEf"
   },
   "source": [
    "# üöã Train Model with W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueJ3cunufae5"
   },
   "outputs": [],
   "source": [
    "early_stopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4mQQ7i5nRU5"
   },
   "outputs": [],
   "source": [
    "wandb.init(project='emnist-nlp')\n",
    "\n",
    "_ = model.fit(trainloader,  \n",
    "                    validation_data=testloader,\n",
    "                    epochs=100,\n",
    "                    callbacks=[WandbCallback(),\n",
    "                               early_stopper])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbOx_WuMtYem"
   },
   "source": [
    "#### ‚úîÔ∏è Save your hard work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QG6ooSDFfqcE"
   },
   "outputs": [],
   "source": [
    "model.save('emnist-nlp.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UngLxdFvtiXt"
   },
   "source": [
    "# üëÄ Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Veszg_ejudSQ"
   },
   "source": [
    "#### ‚úîÔ∏è Simplify Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwpYiNI_uf-i"
   },
   "outputs": [],
   "source": [
    "# notice we are removing CTCLayer\n",
    "inference_model = Model(model.get_layer('cnn_input').input, model.get_layer('lstm_softmax').output)\n",
    "inference_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBZC4UPhwQKg"
   },
   "source": [
    "#### ‚úîÔ∏è Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFyStXxLvI5X"
   },
   "outputs": [],
   "source": [
    "(val_images, val_labels), _ = next(iter(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMQt-H77wwH9"
   },
   "outputs": [],
   "source": [
    "def get_prediction(val_images, val_labels):\n",
    "    # get prediction probability \n",
    "    predictions = inference_model.predict(val_images)\n",
    "    # get argmax\n",
    "    pred_indices = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    actual_text_list = []\n",
    "    pred_text_list = []\n",
    "\n",
    "    for i in range(val_images.shape[0]):\n",
    "        ans = \"\"\n",
    "        pred_ans = \"\"\n",
    "\n",
    "        # get actual text\n",
    "        for p in val_labels[i].numpy():\n",
    "            if p in mappings.keys():\n",
    "              ans+=mappings[p]\n",
    "\n",
    "        # get predicted text from image\n",
    "        merged_list = [k for k,_ in groupby(pred_indices[i])]\n",
    "        for p in merged_list:\n",
    "            if p in mappings.keys():\n",
    "              pred_ans+=mappings[p]\n",
    "\n",
    "        actual_text_list.append(ans)\n",
    "        pred_text_list.append(pred_ans)\n",
    "\n",
    "    return actual_text_list, pred_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7k2LlgTP1TuI"
   },
   "outputs": [],
   "source": [
    "actual_text, predicted_text = get_prediction(val_images, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NfaOBJ6Yiub"
   },
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=[\"Actual Text\", \"Predicted Text\"])\n",
    "for act_text, pred_text in zip(actual_text, predicted_text):\n",
    "    table.add_data(act_text, pred_text)\n",
    "\n",
    "wandb.log({\"emnist-nlp\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wzSdAkt4d_xB"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [wandb\u001b[38;5;241m.\u001b[39mImage(np\u001b[38;5;241m.\u001b[39mexpand_dims(image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m      2\u001b[0m                             \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m val_images]})\n\u001b[0;32m      4\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.log({\"images\": [wandb.Image(np.expand_dims(image, axis=2))\n",
    "                            for image in val_images]})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "lwUb0KtemeK3",
    "USfu5WfonDPZ",
    "TiVoXehQnsOB",
    "75WCj0tmqIwo",
    "ASm9g8-_qsFR",
    "jgEJHI4CsUvY",
    "VmgFAVAFr_1u",
    "XNeF-P0gsit7",
    "wmjDDWLfHWyJ",
    "ezbWeXARO_C0"
   ],
   "name": "EMNIST NLP",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
